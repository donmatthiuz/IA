{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Examen 1"
      ],
      "metadata": {
        "id": "DkUwLGMKYNDM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xt5QJRQIYL76"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definicion del problema"
      ],
      "metadata": {
        "id": "CQ8H7Urhc6bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FrozenLakeMDP:\n",
        "    def __init__(self, nombre_entorno=\"FrozenLake-v1\", es_resbaladizo=True):\n",
        "        self.entorno = gym.make(nombre_entorno, is_slippery=es_resbaladizo)\n",
        "\n",
        "        self.n_estados = self.entorno.observation_space.n  # 16 estados\n",
        "        self.n_acciones = self.entorno.action_space.n  # 4 acciones (izq, abajo, der, arriba)\n",
        "\n",
        "        self.descripcion = \"\"\"\n",
        "        El problema del Lago Congelado es un MDP donde un agente debe llegar a la meta sin caer en agujeros.\n",
        "        - Estados: 16 (correspondientes a un tablero 4x4).\n",
        "        - Acciones: 4 (izquierda, abajo, derecha, arriba).\n",
        "        - Transiciones: Probabil√≠sticas si el hielo es resbaladizo.\n",
        "        - Recompensas: 1 por llegar a la meta, 0 en cualquier otro caso.\n",
        "        \"\"\"\n",
        "\n",
        "        self.gamma = 0.99\n",
        "\n",
        "    def obtener_definicion_problema(self):\n",
        "        return self.descripcion"
      ],
      "metadata": {
        "id": "0uem9RAbc2Yk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Framework MDP"
      ],
      "metadata": {
        "id": "X3CdbvBTc-wN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MDPFramework:\n",
        "    def __init__(self, nombre_entorno=\"FrozenLake-v1\", es_resbaladizo=True, gamma=0.99):\n",
        "        self.entorno = gym.make(nombre_entorno, is_slippery=es_resbaladizo)\n",
        "\n",
        "        self.n_estados = self.entorno.observation_space.n\n",
        "        self.n_acciones = self.entorno.action_space.n\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.transiciones = self._extraer_transiciones()\n",
        "        self.recompensas = self._extraer_recompensas()\n",
        "\n",
        "    def _extraer_transiciones(self):\n",
        "        transiciones = {}\n",
        "        entorno_sin_envuelta = self.entorno.unwrapped\n",
        "\n",
        "        for estado in range(self.n_estados):\n",
        "            transiciones[estado] = {}\n",
        "            for accion in range(self.n_acciones):\n",
        "                transiciones[estado][accion] = entorno_sin_envuelta.P[estado][accion]\n",
        "\n",
        "        return transiciones\n",
        "\n",
        "    def _extraer_recompensas(self):\n",
        "        recompensas = np.zeros((self.n_estados, self.n_acciones))\n",
        "        entorno_sin_envuelta = self.entorno.unwrapped\n",
        "\n",
        "        for estado in range(self.n_estados):\n",
        "            for accion in range(self.n_acciones):\n",
        "                for prob, estado_siguiente, recompensa, terminado in entorno_sin_envuelta.P[estado][accion]:\n",
        "                    recompensas[estado][accion] += prob * recompensa\n",
        "\n",
        "        return recompensas\n",
        "\n",
        "    def obtener_probabilidades_transicion(self, estado, accion):\n",
        "        return self.transiciones[estado][accion]\n",
        "\n",
        "    def obtener_recompensa(self, estado, accion):\n",
        "        return self.recompensas[estado][accion]\n",
        "\n",
        "    def obtener_acciones_posibles(self, estado):\n",
        "        return list(range(self.n_acciones))\n",
        "\n",
        "    def reiniciar(self):\n",
        "        return self.entorno.reset()[0]\n",
        "\n",
        "    def paso(self, accion):\n",
        "        return self.entorno.step(accion)\n",
        "\n",
        "    def cerrar(self):\n",
        "        self.entorno.close()"
      ],
      "metadata": {
        "id": "64vDkdEBeY8i"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}